\chapter[Reflection from a software engineering perspective]{Reflection from a software \\ engineering perspective}
\label{chap:Reflection from a software engineering perspective}

The "Virtual Humans"-project was mostly divided into two separate products.
First of all there was the agent itself, and secondly there was the connector between the environment and the agents.
These two parts will be discussed separately, because the agent was individual effort for each group and the connector was a collaboration between all groups. General software engineering principles will be discussed first,
followed by the software engineering aspects that relate to the two different products specifically.

\section{General}
\label{sec:General}
We were required to use a single communication platform, Slack, in which all teams and their members as well as the teaching assistants were available. 

However, initially we had some start up problems with the project, such as: lack of information, scattered communication and absence of team members. Throughout the project these issues were attempted to be resolved by forcing team members to use Slack as the main communication platform, by making clear appointments and by introducing penalties for absence. Despite efforts being made to resolve these problems we kept struggling with these issues, because several team members had low motivation or had an occupied scheme. Drops in motivation were most likely introduced by multiple issues that kept lingering due to poor communication. Many of these issues were keeping team members behind while they, if directly discussed, could have been resolved before stagnating the working progress. 
For upcoming projects everyone should ensure themselves to have enough time reserved. Also a single communication channel should already be frequently used at the start of any project, so that it will not delay communication later on. 

When we were able to start working on the agent full-time, we had an problem with people that were writing dependencies for other team members without communicating their progress clearly, so the dependent people did not know when they could start working on their part. There were also some issues with improper and dirty branching, but we always managed to resolve these before the next sprint. 

We were required to use the SCRUM-methodology. This states that our product should be in a working state at the end of every sprint, with a sprint being defined as one week.
According to SCRUM, this is realized with the use of backlog at the beginning of each sprint, daily SCRUM meetings during the sprint, and a reflection at the end of the sprint.

As for our scrum management, the backlog was systematically made at the beginning of each sprint and was overall effectively used to divide the tasks for the upcoming sprint. At the beginning of the project, the prioritization of the tasks was not completely up to par, but this was adjusted accordingly in later sprints.
We elected to have the daily SCRUM meetings through voice chat, every day. However the attendance at these meetings was abysmal, often only two or three members of the group were present during these meetings.
Reflections were also made systematically at the end of each week, but the overall quality of the reflections varied quite a lot during the project.
To our displeasure, we were unable to deliver a working agent at the end of some sprints. At times this was caused by factors outside of our control, but it was also due to bad time management,
which caused the need for tasks to be carried over to future sprints.

Additionally a pull-based development model had to be used throughout the development-process. New functionality, bug fixes, documentation etc. were all added through pull requests, and were supposed to only be merged after at least a majority of the participants have reviewed and approved the request. 

Within our group, reviews of pull requests were not always extensive enough, which led to lots of inconsistencies in style, which caused that later on re-factoring had to be done.

Most of the afore mentioned issues would have had significantly less impact if team members showed more dedication at the start of a sprint, because then there would have been more time to correct mistakes and solve issues that arose throughout the project. This would be the best improvement for following projects. 

\section{Private Housing Company Agent}
\label{sec:Private Housing Company Agent}

We were obliged to utilize continuous integration with proper branch management. For that purpose we also had to make use of static analysis tools such as: 'CheckStyle', 'PMD' and 'FindBugs'. Additionally we had to use test driven development. We had to make use of 'Travis' to check each commit on static analysis and tests. 

The agent was developed with the GOAL programming language. The use of GOAL caused a lot of challenges regarding static analysis tools and continuous integration. None of the static analysis tools required for the project worked in conjunction with GOAL. This meant that the 'Travis' configuration used for the agents is very limited.

The test driven development model was completely unused during the project. Lots of functionality was added to the agent without any automatic tests. This was partly caused by bad time management but mostly because of external issues. Tests in GOAL initially did not function properly, which made automatic testing available only past halfway in the project. As soon as the issue was solved, we started adding automatic tests for already existing modules. For multiple modules however, there was either interaction required with other stakeholders or there were multiple connector dependencies, which meant that there was no way to add proper automatic tests for these modules. We managed to get around the lack of tests fairly good as we always had all our code manually tested thoroughly and all encountered issues were usually solved before merging a pull request. Some pull requests however were still improperly tested before the merge, causing team members to spend much time on testing and debugging the code of another member. 

At first, we managed to find a way that Travis would verify that all test cases written would run successfully. However we later discovered that GOAL-tests do not run as intended when ran by Travis. 
Because of this reason, only some specific tests are ran through Travis. 

At the end of the project we provided a product which did satisfy our client. We were not able to implement advanced decision logic or a well defined strategy. 
In sprint three we encountered the largest problem in the project. The connector was embarrassingly less developed than was assumed. The reaction to this was that the following weeks up to week seven was spent mostly to only on working on the connector to get it as far as we required it to be. Despite all efforts being made on the connector, we still ended up with less functionality than we needed to implement our business plan. 

Another issue arose with GOAL debugging, which stopped working and had not been fixed since, so we came up with a workaround to still be able to get some debug information while testing the bot. This caused some performance loss, because testing would take considerably longer.

In sprint six one of our members experienced a large problem with GOAL, because of which we practically lost one member for a week worth of work.

\section{Tygron Connector}
\label{sec:Tygron Connector}

Since the connector was completely written in Java, static analysis tools and continuous integration did not cause issues, unlike with the agents in GOAL. This meant that all required static analysis tools were used extensively, creating a consistent style and level of quality in the connector.

Test driven development was not used in the development of the connector, this was a decision made with all groups, because for the provided connector it was not feasible to obtain a high enough level of understanding to start test driven development given the time-frame. But unlike with the agent, every feature was thoroughly tested automatically before being added to the final product.
